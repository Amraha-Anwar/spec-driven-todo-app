# Implementation Plan: Force Explicit Tool Execution & Eliminate Ghost Success

**Branch**: `015-force-explicit-tool` | **Date**: 2026-02-08 | **Spec**: [specs/015-force-explicit-tool/spec.md](spec.md)

**Input**: Feature specification for eliminating "Ghost Success" and forcing explicit tool execution for CRUD operations. Requirements include: Agent forbidden from synthesizing success without tool execution; ChatService catches None tool_calls; Date synchronized to 2026; Agent uses titles with transparent UUID mapping; Audit AgentRunner.py tool binding; Implement execution guard with retry logic; Ensure case-insensitive fuzzy matching in ReferenceResolver.

---

## Summary

This feature eliminates "Ghost Success" messages (claiming action succeeded when no tool was executed) by implementing three critical safeguards:

1. **Tool Binding Verification** (AgentRunner): Audit and enforce that OpenRouter receives tools array with tool_choice='auto' in every agent request.
2. **Execution Guard** (ChatService): Detect when tool_calls are missing for intent-required operations, retry with forced system instruction.
3. **Reference Mapping** (ReferenceResolver): Implement case-insensitive fuzzy matching to reliably map user-provided task titles to internal UUIDs.

The combined effect ensures users can confidently use natural language ("delete Read book", "list my tasks") with guaranteed tool execution and immediate error reporting if tools fail to invoke.

---

## Technical Context

**Language/Version**: Python 3.11+ (FastAPI ecosystem)
**Primary Dependencies**: FastAPI, SQLModel, OpenRouter API, Official MCP SDK, asyncio
**Storage**: Neon PostgreSQL (existing)
**Testing**: pytest for unit/integration tests
**Target Platform**: Linux server (Docker)
**Project Type**: Backend service (monolithic FastAPI)
**Performance Goals**: <500ms p95 latency for chat endpoint (including agent call), zero silent failures
**Constraints**: Stateless architecture (no in-memory state), user isolation via JWT, tool_choice='auto' enforcement
**Scale/Scope**: Single user to multi-user, 7 tasks per user (prototype), extensible to 10k+ tasks

---

## Constitution Check

**GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.**

| Principle | Requirement | Status | Notes |
|-----------|------------|--------|-------|
| I. Stateless Architecture | No in-memory state; all context to DB | ✅ PASS | ChatService maintains no state between requests; tool execution is logged to Message table |
| II. Tool-First Execution | All CRUD via MCP tools, never direct DB | ✅ PASS | Execution guard enforces tool invocation; no direct DB queries in this feature |
| III. Privacy & Isolation | user_id verification in all tool calls | ✅ PASS | _resolve_task_reference validates user_id; all tool calls include user_id parameter |
| IV. API Integration & Cost Efficiency | OpenRouter API with centralized config | ✅ PASS | AgentRunner audited for OpenRouter compatibility; tool_choice='auto' standardized |
| V. User Experience & Clarity | Friendly responses, error handling clarity | ✅ PASS | action_metadata provides clear success/error feedback; no cryptic system messages |
| VI. Data Integrity & Observability | Persistent logging with timestamps | ✅ PASS | Tool execution logged to Message.tool_call_metadata; timestamps on all records |

**GATE RESULT**: ✅ PASS - All constitution principles satisfied. No violations or waivers needed.

---

## Project Structure

### Documentation (this feature)

```text
specs/015-force-explicit-tool/
├── spec.md                          # Feature specification (completed)
├── plan.md                          # This file (implementation plan)
├── research.md                      # Phase 0 (to be generated)
├── data-model.md                    # Phase 1 (to be generated)
├── contracts/                       # Phase 1 (to be generated)
│   ├── agent_runner_request.json    # OpenRouter payload schema
│   └── chat_service_response.json   # Chat endpoint response schema
├── quickstart.md                    # Phase 1 (to be generated)
├── checklists/
│   └── requirements.md              # Quality checklist (completed)
└── tasks.md                         # Phase 2 (to be generated by /sp.tasks)
```

### Source Code (existing structure, no new directories)

```text
backend/
├── src/
│   ├── services/
│   │   ├── agent_runner.py         # AUDIT: Tool binding verification (FIX #1)
│   │   ├── chat_service.py         # MODIFY: Execution guard + None detection (FIX #2)
│   │   └── ... (existing)
│   ├── tools/
│   │   ├── reference_resolver.py   # MODIFY: Case-insensitive fuzzy match (FIX #3)
│   │   ├── task_toolbox.py         # (no changes needed)
│   │   └── ... (existing)
│   ├── api/
│   │   ├── chat.py                 # (no changes needed; schema already updated)
│   │   └── ... (existing)
│   └── models/
│       ├── task.py                 # (no changes)
│       └── ... (existing)
└── tests/
    └── unit/
        ├── test_agent_runner_tool_binding.py   # NEW: Verify tool binding
        ├── test_chat_service_execution_guard.py # NEW: Test retry logic
        └── test_reference_resolver_fuzzy.py     # NEW: Test fuzzy matching
```

**Structure Decision**: Existing monolithic FastAPI backend is appropriate. Feature modifications are localized to 3 service files + 3 new test files. No new directories or architectural changes needed.

---

## Complexity Tracking

No constitution violations detected. No complexity waivers needed.

---

## Architecture Decisions

### Decision 1: AgentRunner Tool Binding (FR-012, FR-013)

**Problem**: Need to verify OpenRouter receives `tools` array and `tool_choice='auto'` in every agent request.

**Options Considered**:
- A) Audit existing AgentRunner.py and add explicit validation/logging
- B) Create new ToolBindingValidator class with separate concerns
- C) Add validation inline within run_agent method

**Chosen**: Option A (audit + enhance existing run_agent method)

**Rationale**:
- AgentRunner is already responsible for agent orchestration
- Minimal diff, leverages existing error handling
- Logging is already present; just enhance it
- Validation can be inline check before payload construction

**Tradeoffs**:
- Validation logic lives in one place (good)
- vs. Creating separate validator (overhead for simple check)

**Implementation**:
1. Audit current AgentRunner.run_agent() method
2. Verify tools array is passed from ChatService
3. Verify tool_choice='auto' is hardcoded in OpenRouter payload
4. Add debug log: "Tools array: [count] tools, tool_choice='auto'" before API call
5. Add error if tools array is empty or None (return error to ChatService)

---

### Decision 2: ChatService Execution Guard with Retry (FR-001, FR-002)

**Problem**: Need to detect when agent returns no tool_calls for an operation that clearly requires one (add, delete, update, list).

**Options Considered**:
- A) Block response synthesis immediately, return error "Technical error: Tool not triggered"
- B) Retry once with forced system instruction appended to retry
- C) Keep attempting retries until success or max attempts reached

**Chosen**: Option B (single retry with forced system instruction)

**Rationale**:
- Single retry provides one chance for agent to correct without excessive latency
- "Forced system instruction" is a proven technique to increase compliance
- User gets clear error if even retry fails (bounded latency)
- Prevents infinite loops or excessive API calls

**Tradeoffs**:
- B vs. A: Retry gives agent one more chance (better UX) but doubles latency on failure (worse worst-case)
- B vs. C: Single retry is deterministic (max 2 API calls) vs. unknown number of attempts

**Implementation**:
1. After agent returns response, check tool_calls array
2. Infer expected tool from user message (intent classification: add/delete/update/list)
3. If tool_calls is empty/None AND expected tool detected:
   - Log: "Tool execution guard: Missing [tool_name], retrying with forced instruction"
   - Append system instruction: "CRITICAL: You MUST call the [tool_name] tool for this request. Do NOT respond without calling it."
   - Call agent again (2nd attempt)
4. If retry still returns no tool_calls, return error: "Technical error: Tool not triggered."
5. If retry succeeds, proceed with execution

**Intent Classification** (how we know which tool is required):
- User message contains "delete" OR "remove" → expect delete_task
- User message contains "list" OR "show" OR "what are" → expect list_tasks
- User message contains "add" OR "create" OR "new task" → expect add_task
- User message contains "mark" OR "done" OR "complete" → expect complete_task
- User message contains "update" OR "change" OR "set" → expect update_task

---

### Decision 3: ReferenceResolver Case-Insensitive Fuzzy Match (FR-010, FR-011)

**Problem**: Need reliable mapping of user-provided task titles (e.g., "Read book") to internal UUIDs without false mismatches.

**Options Considered**:
- A) Exact case-sensitive match only (strict, high false-negative rate)
- B) Case-insensitive exact match only (better, but "read books" ≠ "Read book")
- C) Case-insensitive fuzzy match with score threshold (flexible, low false-negatives)

**Chosen**: Option C (case-insensitive fuzzy match with score threshold)

**Rationale**:
- Real users don't always use exact casing or pluralization
- Fuzzy matching (e.g., difflib.SequenceMatcher) is proven technique
- Score threshold (e.g., >0.6) prevents false positives
- Handles accidental typos ("Rad book" → "Read book")

**Tradeoffs**:
- C vs. A: More lenient, better UX, but risk of matching wrong task if titles are very similar
- C vs. B: Handles minor variations, but adds complexity

**Implementation**:
1. In ReferenceResolver.resolve_reference():
   - Convert user reference and all task titles to lowercase for comparison
   - Try exact match first: if task.title.lower() == reference.lower(), return immediately
   - If no exact match, iterate all tasks and calculate fuzzy match score
   - Use difflib.SequenceMatcher(None, reference.lower(), task.title.lower()).ratio()
   - Collect all tasks with score >= 0.6 (configurable threshold)
   - If exactly one match found, return it
   - If zero matches, return error "No task matching '[reference]' found"
   - If multiple matches, return error "Multiple tasks match '[reference]'. Available: [list]" and ask user to clarify

**Score Interpretation**:
- 1.0 = exact match
- 0.8+ = likely match (e.g., "Read book" vs "Read books")
- 0.6-0.8 = possible match (e.g., "Red book" vs "Read book")
- <0.6 = unlikely match

---

### Decision 4: System Prompt Enhancement for Date Context (FR-006, FR-007)

**Problem**: Prevent LLM from hallucinating past dates (2024, 2025) or future dates (2027+).

**Options Considered**:
- A) Mention current date once in system prompt
- B) Mention current date multiple times (in context + rules section)
- C) Include specific forbidden dates + explicit 2026 requirement

**Chosen**: Option C (current approach from prior implementation)

**Rationale**:
- Research shows repetition reduces temporal hallucinations
- Explicit forbidden dates are clearer than general rules
- Current implementation (Phase 024) already has this; maintain it

**Tradeoffs**:
- Added verbosity in system prompt (minor)
- vs. Single mention (insufficient based on observed hallucinations)

**Implementation**: Already implemented in prior phase (024). No changes needed. Verify during execution guard retry (include date context in forced instruction).

---

### Decision 5: Action Metadata Structure (FR-004)

**Problem**: Frontend needs to display task operation results (success/failure) with clear action feedback.

**Options Considered**:
- A) Embed metadata in assistant_message string (fragile, hard to parse)
- B) Separate action_metadata field in API response (structured, type-safe)
- C) Separate metadata field PLUS separate messages array (duplicative)

**Chosen**: Option B (already implemented in Phase 024)

**Rationale**:
- Structured data is reliable and frontend-friendly
- action_metadata field exists and is being populated
- Keeps responsibility separation: assistant_message for human, action_metadata for frontend

**Implementation**: Already done. Verify during testing that action_metadata is always populated on tool success.

---

## Data Model

### Existing Models (No Changes)

**Task**:
- id: UUID (primary key)
- user_id: str (foreign key, isolation)
- title: str (required, task name)
- description: str (optional)
- status: str (pending/completed)
- priority: str (low/medium/high)
- due_date: date (optional, must be 2026)
- created_at: datetime
- updated_at: datetime

**Message**:
- id: UUID (primary key)
- conversation_id: UUID (foreign key)
- role: str (user/assistant)
- content: str (message text)
- tool_call_metadata: dict (execution records)
- created_at: datetime

### New Metadata Structures (In Response)

**ToolCallRecord**:
```json
{
  "name": "delete_task",
  "arguments": {"user_id": "user-123", "task_id": "uuid-xxx"},
  "result": {
    "success": true,
    "data": {
      "id": "uuid-xxx",
      "title": "Read book",
      "status": "deleted"
    }
  },
  "retried": false,
  "retry_reason": null
}
```

**ActionMetadata**:
```json
{
  "action": "delete_task",
  "success": true,
  "task_id": "uuid-xxx",
  "task_title": "Read book",
  "message": "Deleted: Read book"
}
```

---

## Contracts & API

### Chat Endpoint Response (Enhanced)

**POST /api/{user_id}/chat** (existing, enhanced)

**Response Schema** (updated):
```json
{
  "conversation_id": "uuid",
  "assistant_message": "I've deleted that task for you.",
  "tool_calls": [
    {
      "name": "delete_task",
      "arguments": {"user_id": "user-123", "task_id": "uuid-xxx"}
    }
  ],
  "messages": [...],
  "action_metadata": {
    "action": "delete_task",
    "success": true,
    "task_id": "uuid-xxx",
    "task_title": "Read book",
    "message": "Deleted: Read book"
  },
  "execution_metadata": {
    "tool_calls_executed": 1,
    "tool_calls_retried": 0,
    "total_latency_ms": 450
  }
}
```

**New Field: execution_metadata** (for debugging):
- tool_calls_executed: count of successful tool invocations
- tool_calls_retried: count of retries due to missing tools
- total_latency_ms: end-to-end latency (agent + tools + synthesis)

---

## Error Handling & Recovery

### Scenario 1: Tool Not Called (Missing tool_calls)

**Flow**:
1. User: "delete Read book"
2. Agent response: `tool_calls=[]` (empty)
3. ChatService detects: "delete" keyword → expect delete_task
4. Missing tool detected → Execute guard triggers
5. Append forced instruction: "CRITICAL: You MUST call the delete_task tool..."
6. Retry agent (attempt 2)
7. **Outcome A (Success)**: Agent calls delete_task → execute and respond
8. **Outcome B (Failure)**: Agent still doesn't call tool → return error "Technical error: Tool not triggered."

### Scenario 2: Tool Called but Returns Error

**Flow**:
1. User: "delete NonexistentTask"
2. Agent calls: delete_task(task_id="NonexistentTask")
3. delete_task returns: `{"success": false, "error": "Task not found"}`
4. ChatService detects: tool_calls[0].result.success == false
5. Use error message in synthesis: "I couldn't find a task named 'NonexistentTask'..."
6. action_metadata: `{"action": "delete_task", "success": false, ...}`
7. **Outcome**: User sees clear error, not fake success

### Scenario 3: Reference Resolution Ambiguous

**Flow**:
1. User: "delete book" (but 3 tasks contain "book": "Read book", "Return book to library", "Book list")
2. ReferenceResolver.resolve_reference("book") → multiple matches
3. Return error: "Multiple tasks match 'book'. Please be more specific: Read book, Return book to library, Book list"
4. ChatService reports to user: [error message with suggestions]
5. User clarifies: "delete Read book"
6. Resolution succeeds on 2nd attempt

---

## Testing Strategy

### Unit Tests

**Test File**: backend/tests/unit/test_agent_runner_tool_binding.py
- Verify AgentRunner logs "Tools array: N tools, tool_choice='auto'" before API call
- Verify error if tools array is empty
- Verify tool_choice is always 'auto' in payload

**Test File**: backend/tests/unit/test_chat_service_execution_guard.py
- Test intent detection: "delete" → expect delete_task tool
- Test missing tool detection: tool_calls=[] with expected tool
- Test retry with forced instruction appended
- Test success after retry
- Test error after 2nd attempt still fails
- Test action_metadata populated correctly

**Test File**: backend/tests/unit/test_reference_resolver_fuzzy.py
- Test exact match (case-insensitive): "Read book" vs "read book" → match
- Test fuzzy match: "Read books" vs "Read book" → match (score>0.6)
- Test no match: "Xyz" vs [list of tasks] → error
- Test multiple matches: "book" vs ["Read book", "Return book"] → error with suggestions
- Test typo tolerance: "Rad book" vs "Read book" → match

### Integration Tests

**Test Flow**: End-to-end chat request
1. Create task "Read book"
2. Send "delete Read book"
3. Verify delete_task tool in logs
4. Verify task removed from database
5. Verify action_metadata present in response

**Test Flow**: Retry on missing tool
1. Mock agent to return empty tool_calls on 1st attempt
2. Verify retry with forced instruction
3. Verify success on 2nd attempt

---

## Acceptance Validation

**AC-001**: "delete Read book" triggers delete_task tool → ✅ Testable via logs
**AC-002**: "list my tasks" triggers list_tasks tool → ✅ Testable via logs
**AC-003**: 7 tasks returned → ✅ Verify response.messages[0].content contains 7 task titles
**AC-004**: No ghost success → ✅ All success messages backed by tool execution with success=true
**AC-005**: Tool not called → ✅ Error "Technical error: Tool not triggered." returned

---

## Implementation Phases

### Phase 0: Research (Completed ✅)
- All technical context clarified
- Architecture decisions documented
- Constitution check passed

### Phase 1: Design & Contracts
- Create research.md with detailed findings
- Create data-model.md documenting all structures
- Create contracts/ directory with JSON schemas
- Create quickstart.md with developer quick-start
- Update agent context with new technologies/approaches

### Phase 2: Implementation Tasks
- Generate tasks.md via `/sp.tasks` command
- Define 3 implementation tasks (AgentRunner, ChatService, ReferenceResolver)
- Define 3 test tasks (unit tests for each component)
- Define 1 integration test task
- Define 1 verification task (acceptance criteria validation)

### Phase 3: Execution (Red/Green/Refactor)
- Implement each task in order
- Run tests after each task
- Create PHRs for each implementation turn

---

## Risk Analysis

### Risk 1: Retry Latency Doubles on Tool Failure

**Impact**: User waits ~1000ms (2 agent calls) if first attempt has missing tool

**Mitigation**:
- Agent should succeed on first attempt (well-trained with system prompt)
- Retry is rare edge case
- Timeout on 2nd attempt to prevent runaway

**Acceptance**: Acceptable - better delayed response than silent failure

### Risk 2: Fuzzy Matching False Positives

**Impact**: User "delete book" matches wrong task ("Return book to library" instead of "Read book")

**Mitigation**:
- Score threshold 0.6 is conservative
- Multiple matches return error with list, user must clarify
- Exact match tried first (no ambiguity if exact match exists)

**Acceptance**: Acceptable - user can clarify; no data loss

### Risk 3: Date Context Doesn't Prevent All Hallucinations

**Impact**: Agent still creates tasks with 2024/2025 dates occasionally

**Mitigation**:
- System prompt forbids those dates explicitly
- Retry instruction includes date context
- Validation in add_task checks due_date year

**Acceptance**: Acceptable - add_task validation prevents persistence; user sees error

---

## Success Metrics

After implementation:

1. **Tool Execution Rate**: 100% of CRUD requests result in tool_calls (measured via logs)
2. **Ghost Success Elimination**: 0 instances of success message without tool execution
3. **Retry Success Rate**: >95% of missing-tool cases succeed on retry (target efficiency)
4. **Fuzzy Match Accuracy**: >99% of valid task names resolved correctly (test suite validation)
5. **Date Accuracy**: 100% of created tasks have due_date in 2026 (database validation)
6. **Latency**: <500ms p95 for successful requests (excluding agent call time)

---

## Next Phase

This plan is complete and ready for `/sp.tasks` command to generate detailed, ordered implementation tasks with acceptance scenarios.

Run: `cd phase_03 && /sp.tasks`

Expected output:
- tasks.md with 8-10 implementation tasks
- Task dependencies (AgentRunner before ChatService, etc.)
- Test acceptance scenarios for each task
